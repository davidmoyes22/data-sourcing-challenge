


# Dependencies
import requests
import time
from dotenv import load_dotenv
import os
import pandas as pd
import json


# Set environment variables from the .env in the local environment
load_dotenv()

nyt_api_key = os.getenv("NYT_API_KEY")
tmdb_api_key = os.getenv("TMDB_API_KEY")





# Set the base URL
url = "https://api.nytimes.com/svc/search/v2/articlesearch.json?"

# Filter for movie reviews with "love" in the headline
# section_name should be "Movies"
# type_of_material should be "Review"
filter_query = 'section_name:"Movies" AND type_of_material:"Review" AND headline:"love"'

# Use a sort filter, sort by newest
sort = "newest"

# Select the following fields to return:
# headline, web_url, snippet, source, keywords, pub_date, byline, word_count
field_list = "headline,web_url,snippet,source,keywords,pub_date,byline,word_count"

# Search for reviews published between a begin and end date
begin_date = "20130101"
end_date = "20230531"

# Build URL
#nyt_url = (
    #f"{url}fq={filter_query}&sort={sort}&fl={field_list}&"
   # f"begin_date={begin_date}&end_date={end_date}&api-key={nyt_api_key}"
#)

nyt_url = (
    f"{url}api-key={nyt_api_key}&begin_date={begin_date}&end_date={end_date}"
    + f'&fq={filter_query}&sort={sort}&fl={field_list}'
)


# Create an empty list to store the reviews
reviews_list = []

# loop through pages 0-19
for i in range(0,20):
    # create query with a page number
    # API results show 10 articles at a time
    nyt_p_url = f"{nyt_url}page={i}"
    
    # Make a "GET" request and retrieve the JSON
    reviews = requests.get(nyt_p_url).json()
    
    # Add a twelve second interval between queries to stay within API query limits
    time.sleep(12)
    print("The application is sleeping for 12 seconds")
    
    # Try and save the reviews to the reviews_list
    try:
        # loop through the reviews["response"]["docs"] and append each review to the list
        for review in reviews["response"]["docs"]:
            reviews_list.append(review)
        # Print the page that was just retrieved
        print(f"Checked page {i}")
    except:
        # Print the page number that had no results then break from the loop
        print(f"Page {i} had no results")
        break


# Preview the first 5 results in JSON format
# Use json.dumps with argument indent=4 to format data
print(json.dumps(reviews_list[:5], indent=4))


# Convert reviews_list to a Pandas DataFrame using json_normalize()
reviews_list_df = pd.json_normalize(reviews_list)
reviews_list_df


# Extract the title from the "headline.main" column and
# save it to a new column "title"
# Title is between unicode characters \u2018 and \u2019. 
# End string should include " Review" to avoid cutting title early
reviews_list_df["title"] = reviews_list_df["headline.main"].apply(lambda st: st[st.find("\u2018")+1:st.find("\u2019 Review")])
reviews_list_df


# Extract 'name' and 'value' from items in "keywords" column
def extract_keywords(keyword_list):
    extracted_keywords = ""
    for item in keyword_list:
        # Extract 'name' and 'value'
        keyword = f"{item['name']}: {item['value']};" 
        # Append the keyword item to the extracted_keywords list
        extracted_keywords += keyword
    return extracted_keywords

# Fix the "keywords" column by converting cells from a list to a string
reviews_list_df["keywords"] = reviews_list_df["keywords"].apply(extract_keywords)
reviews_list_df


# Create a list from the "title" column using to_list()
# These titles will be used in the query for The Movie Database
title_list = reviews_list_df["title"].to_list()
title_list





# Prepare The Movie Database query
url = "https://api.themoviedb.org/3/search/movie?query="
tmdb_key_string = "&api_key=" + tmdb_api_key


# Create an empty list to store the results
tmdb_movies_list = []

# Create a request counter to sleep the requests after a multiple
# of 50 requests
request_counter = 1

# Loop through the titles
for title in title_list:
    # Check if we need to sleep before making a request
    if request_counter % 50 == 0:
        time.sleep(1)
        print(f"The application is sleeping {request_counter}")

    # Add 1 to the request counter
    request_counter += 1
    
    # Perform a "GET" request for The Movie Database
    # movie_data = requests.get(f"{url}{title}{tmdb_key_string}").json
    movie_data = requests.get(url + title + tmdb_key_string)
    movie_data_id_request = movie_data.json()
    
    # Include a try clause to search for the full movie details.
    # Use the except clause to print out a statement if a movie
    # is not found.
    try:
        # Get movie id
        movie_id = movie_data_id_request["results"][0]["id"]

        # Make a request for a the full movie details
        #new_url = "https://api.themoviedb.org/3/movie/"
        #movie_id_url = f"{new_url}{movie_id}{tmdb_key_string}"
        movie_id_url = f"https://api.themoviedb.org/3/movie/{movie_id}?api_key={tmdb_api_key}"
        
        # Execute "GET" request with url
        movie_id_request = requests.get(movie_id_url).json()
        
        # Extract the genre names into a list
        genres = []
        for genre in movie_id_request['genres']:
            genres.append(
                genre["name"]
            ) 

        # Extract the spoken_languages' English name into a list
        spoken_languages = []
        for lang in movie_id_request['spoken_languages']:
            spoken_languages.append(
                lang["english_name"]
            ) 

        # Extract the production_countries' name into a list
        production_countries = []
        for country in movie_id_request['production_countries']:
            production_countries.append(
                country["name"]
            ) 

        # Add the relevant data to a dictionary and
        # append it to the tmdb_movies_list list
        tmdb_movies_list.append(
            {
            "title": movie_id_request['title'],
            "original_title": movie_id_request['original_title'],
            "budget": movie_id_request['budget'],
            "original_language": movie_id_request['original_language'],
            "homepage": movie_id_request['homepage'],
            "overview": movie_id_request['overview'],
            "popularity": movie_id_request['popularity'],
            "runtime": movie_id_request['runtime'],
            "revenue": movie_id_request['revenue'],
            "release_date": movie_id_request['release_date'],
            "vote_average": movie_id_request['vote_average'],
            "vote_count": movie_id_request['vote_count'],
            "genres": genres,
            "spoken_languages": spoken_languages,
            "production_countries": production_countries
            }
        )
        print(f"Found {title}")
    except:   
        # Print out the title that was not found
        print(f"{title} was not found")



# Preview the first 5 results in JSON format
# Use json.dumps with argument indent=4 to format data
print(json.dumps(tmdb_movies_list[:5], indent=4))


# Convert the results to a DataFrame
movie_df = pd.DataFrame(tmdb_movies_list)
movie_df





# Merge the New York Times reviews and TMDB DataFrames on title
merged_df = pd.merge(movie_df, reviews_list_df, on="title")
merged_df


# Remove list brackets and quotation marks on the columns containing lists
# Create a list of the columns that need fixing
columns_to_fix = ["genres", "spoken_languages", "production_countries"]

# Create a list of characters to remove
characters_to_remove = ["[", "]", "'"]

# Loop through the list of columns to fix
for col in columns_to_fix:
    # Convert the column to type 'str'


    # Loop through characters to remove
    for char in characters_to_remove
        

# Display the fixed DataFrame



# Drop "byline.person" column
movie_df = movie_df.drop(['byline.person'], axis=1)


# Delete duplicate rows and reset index
movie_df = movie_df.reset_index(drop=True)



# Export data to CSV without the index
df.to_csv("collected_data.csv", index=False)



